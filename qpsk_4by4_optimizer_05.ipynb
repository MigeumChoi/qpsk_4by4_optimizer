{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUhH1gFREwcHQK+56XvYCd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"x_XT2jTyUm98","executionInfo":{"status":"ok","timestamp":1695544605477,"user_tz":-540,"elapsed":89299,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"663e0a31-0a48-44b4-e6d2-d3817cfb10a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3668\n","Epoch 1: val_loss improved from inf to 0.34940, saving model to hl5_0100.h5\n","1/1 [==============================] - 1s 1s/step - loss: 0.3668 - val_loss: 0.3494\n","Epoch 2/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3630\n","Epoch 2: val_loss improved from 0.34940 to 0.34588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.3630 - val_loss: 0.3459\n","Epoch 3/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3592\n","Epoch 3: val_loss improved from 0.34588 to 0.34243, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.3592 - val_loss: 0.3424\n","Epoch 4/3000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\r1/1 [==============================] - ETA: 0s - loss: 0.3555\n","Epoch 4: val_loss improved from 0.34243 to 0.33904, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.3555 - val_loss: 0.3390\n","Epoch 5/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3519\n","Epoch 5: val_loss improved from 0.33904 to 0.33571, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.3519 - val_loss: 0.3357\n","Epoch 6/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3483\n","Epoch 6: val_loss improved from 0.33571 to 0.33242, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.3483 - val_loss: 0.3324\n","Epoch 7/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3448\n","Epoch 7: val_loss improved from 0.33242 to 0.32919, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.3448 - val_loss: 0.3292\n","Epoch 8/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3413\n","Epoch 8: val_loss improved from 0.32919 to 0.32600, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.3413 - val_loss: 0.3260\n","Epoch 9/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3379\n","Epoch 9: val_loss improved from 0.32600 to 0.32285, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.3379 - val_loss: 0.3228\n","Epoch 10/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3345\n","Epoch 10: val_loss improved from 0.32285 to 0.31973, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.3345 - val_loss: 0.3197\n","Epoch 11/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3312\n","Epoch 11: val_loss improved from 0.31973 to 0.31665, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.3312 - val_loss: 0.3167\n","Epoch 12/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3279\n","Epoch 12: val_loss improved from 0.31665 to 0.31361, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.3279 - val_loss: 0.3136\n","Epoch 13/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3246\n","Epoch 13: val_loss improved from 0.31361 to 0.31060, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.3246 - val_loss: 0.3106\n","Epoch 14/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3214\n","Epoch 14: val_loss improved from 0.31060 to 0.30763, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.3214 - val_loss: 0.3076\n","Epoch 15/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3182\n","Epoch 15: val_loss improved from 0.30763 to 0.30470, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.3182 - val_loss: 0.3047\n","Epoch 16/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3151\n","Epoch 16: val_loss improved from 0.30470 to 0.30180, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.3151 - val_loss: 0.3018\n","Epoch 17/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3120\n","Epoch 17: val_loss improved from 0.30180 to 0.29893, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.3120 - val_loss: 0.2989\n","Epoch 18/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3089\n","Epoch 18: val_loss improved from 0.29893 to 0.29610, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.3089 - val_loss: 0.2961\n","Epoch 19/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3059\n","Epoch 19: val_loss improved from 0.29610 to 0.29330, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.3059 - val_loss: 0.2933\n","Epoch 20/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3028\n","Epoch 20: val_loss improved from 0.29330 to 0.29053, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.3028 - val_loss: 0.2905\n","Epoch 21/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2999\n","Epoch 21: val_loss improved from 0.29053 to 0.28779, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2999 - val_loss: 0.2878\n","Epoch 22/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2969\n","Epoch 22: val_loss improved from 0.28779 to 0.28508, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2969 - val_loss: 0.2851\n","Epoch 23/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2940\n","Epoch 23: val_loss improved from 0.28508 to 0.28241, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.2940 - val_loss: 0.2824\n","Epoch 24/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2912\n","Epoch 24: val_loss improved from 0.28241 to 0.27976, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.2912 - val_loss: 0.2798\n","Epoch 25/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2883\n","Epoch 25: val_loss improved from 0.27976 to 0.27713, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.2883 - val_loss: 0.2771\n","Epoch 26/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2855\n","Epoch 26: val_loss improved from 0.27713 to 0.27454, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2855 - val_loss: 0.2745\n","Epoch 27/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2827\n","Epoch 27: val_loss improved from 0.27454 to 0.27196, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2827 - val_loss: 0.2720\n","Epoch 28/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2799\n","Epoch 28: val_loss improved from 0.27196 to 0.26942, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2799 - val_loss: 0.2694\n","Epoch 29/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2772\n","Epoch 29: val_loss improved from 0.26942 to 0.26690, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.2772 - val_loss: 0.2669\n","Epoch 30/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2745\n","Epoch 30: val_loss improved from 0.26690 to 0.26441, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.2745 - val_loss: 0.2644\n","Epoch 31/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2718\n","Epoch 31: val_loss improved from 0.26441 to 0.26194, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.2718 - val_loss: 0.2619\n","Epoch 32/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2691\n","Epoch 32: val_loss improved from 0.26194 to 0.25950, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2691 - val_loss: 0.2595\n","Epoch 33/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2665\n","Epoch 33: val_loss improved from 0.25950 to 0.25707, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.2665 - val_loss: 0.2571\n","Epoch 34/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2638\n","Epoch 34: val_loss improved from 0.25707 to 0.25467, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2638 - val_loss: 0.2547\n","Epoch 35/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2612\n","Epoch 35: val_loss improved from 0.25467 to 0.25229, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.2612 - val_loss: 0.2523\n","Epoch 36/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2586\n","Epoch 36: val_loss improved from 0.25229 to 0.24994, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.2586 - val_loss: 0.2499\n","Epoch 37/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2561\n","Epoch 37: val_loss improved from 0.24994 to 0.24762, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.2561 - val_loss: 0.2476\n","Epoch 38/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2536\n","Epoch 38: val_loss improved from 0.24762 to 0.24532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.2536 - val_loss: 0.2453\n","Epoch 39/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2511\n","Epoch 39: val_loss improved from 0.24532 to 0.24305, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.2511 - val_loss: 0.2431\n","Epoch 40/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2486\n","Epoch 40: val_loss improved from 0.24305 to 0.24080, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2486 - val_loss: 0.2408\n","Epoch 41/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2462\n","Epoch 41: val_loss improved from 0.24080 to 0.23858, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2462 - val_loss: 0.2386\n","Epoch 42/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2438\n","Epoch 42: val_loss improved from 0.23858 to 0.23638, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2438 - val_loss: 0.2364\n","Epoch 43/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2414\n","Epoch 43: val_loss improved from 0.23638 to 0.23421, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.2414 - val_loss: 0.2342\n","Epoch 44/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2390\n","Epoch 44: val_loss improved from 0.23421 to 0.23205, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.2390 - val_loss: 0.2321\n","Epoch 45/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2367\n","Epoch 45: val_loss improved from 0.23205 to 0.22992, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2367 - val_loss: 0.2299\n","Epoch 46/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2343\n","Epoch 46: val_loss improved from 0.22992 to 0.22782, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2343 - val_loss: 0.2278\n","Epoch 47/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2321\n","Epoch 47: val_loss improved from 0.22782 to 0.22573, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2321 - val_loss: 0.2257\n","Epoch 48/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2298\n","Epoch 48: val_loss improved from 0.22573 to 0.22368, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.2298 - val_loss: 0.2237\n","Epoch 49/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2276\n","Epoch 49: val_loss improved from 0.22368 to 0.22164, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.2276 - val_loss: 0.2216\n","Epoch 50/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2253\n","Epoch 50: val_loss improved from 0.22164 to 0.21963, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.2253 - val_loss: 0.2196\n","Epoch 51/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2231\n","Epoch 51: val_loss improved from 0.21963 to 0.21764, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2231 - val_loss: 0.2176\n","Epoch 52/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2210\n","Epoch 52: val_loss improved from 0.21764 to 0.21567, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.2210 - val_loss: 0.2157\n","Epoch 53/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2188\n","Epoch 53: val_loss improved from 0.21567 to 0.21372, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.2188 - val_loss: 0.2137\n","Epoch 54/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2167\n","Epoch 54: val_loss improved from 0.21372 to 0.21180, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.2167 - val_loss: 0.2118\n","Epoch 55/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2146\n","Epoch 55: val_loss improved from 0.21180 to 0.20989, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2146 - val_loss: 0.2099\n","Epoch 56/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2125\n","Epoch 56: val_loss improved from 0.20989 to 0.20801, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.2125 - val_loss: 0.2080\n","Epoch 57/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2105\n","Epoch 57: val_loss improved from 0.20801 to 0.20615, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.2105 - val_loss: 0.2062\n","Epoch 58/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2084\n","Epoch 58: val_loss improved from 0.20615 to 0.20431, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2084 - val_loss: 0.2043\n","Epoch 59/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2064\n","Epoch 59: val_loss improved from 0.20431 to 0.20249, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2064 - val_loss: 0.2025\n","Epoch 60/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2044\n","Epoch 60: val_loss improved from 0.20249 to 0.20069, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2044 - val_loss: 0.2007\n","Epoch 61/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2025\n","Epoch 61: val_loss improved from 0.20069 to 0.19892, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2025 - val_loss: 0.1989\n","Epoch 62/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2005\n","Epoch 62: val_loss improved from 0.19892 to 0.19716, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.2005 - val_loss: 0.1972\n","Epoch 63/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1986\n","Epoch 63: val_loss improved from 0.19716 to 0.19542, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1986 - val_loss: 0.1954\n","Epoch 64/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1967\n","Epoch 64: val_loss improved from 0.19542 to 0.19370, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1967 - val_loss: 0.1937\n","Epoch 65/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1948\n","Epoch 65: val_loss improved from 0.19370 to 0.19200, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1948 - val_loss: 0.1920\n","Epoch 66/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1929\n","Epoch 66: val_loss improved from 0.19200 to 0.19032, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.1929 - val_loss: 0.1903\n","Epoch 67/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1911\n","Epoch 67: val_loss improved from 0.19032 to 0.18866, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1911 - val_loss: 0.1887\n","Epoch 68/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1893\n","Epoch 68: val_loss improved from 0.18866 to 0.18701, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1893 - val_loss: 0.1870\n","Epoch 69/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1875\n","Epoch 69: val_loss improved from 0.18701 to 0.18539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1875 - val_loss: 0.1854\n","Epoch 70/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1857\n","Epoch 70: val_loss improved from 0.18539 to 0.18378, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1857 - val_loss: 0.1838\n","Epoch 71/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1839\n","Epoch 71: val_loss improved from 0.18378 to 0.18219, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1839 - val_loss: 0.1822\n","Epoch 72/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1822\n","Epoch 72: val_loss improved from 0.18219 to 0.18062, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1822 - val_loss: 0.1806\n","Epoch 73/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1804\n","Epoch 73: val_loss improved from 0.18062 to 0.17906, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1804 - val_loss: 0.1791\n","Epoch 74/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1787\n","Epoch 74: val_loss improved from 0.17906 to 0.17753, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1787 - val_loss: 0.1775\n","Epoch 75/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1770\n","Epoch 75: val_loss improved from 0.17753 to 0.17601, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1770 - val_loss: 0.1760\n","Epoch 76/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1753\n","Epoch 76: val_loss improved from 0.17601 to 0.17451, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1753 - val_loss: 0.1745\n","Epoch 77/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1737\n","Epoch 77: val_loss improved from 0.17451 to 0.17302, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.1737 - val_loss: 0.1730\n","Epoch 78/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1720\n","Epoch 78: val_loss improved from 0.17302 to 0.17155, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1720 - val_loss: 0.1716\n","Epoch 79/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1704\n","Epoch 79: val_loss improved from 0.17155 to 0.17010, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.1704 - val_loss: 0.1701\n","Epoch 80/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1688\n","Epoch 80: val_loss improved from 0.17010 to 0.16867, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1688 - val_loss: 0.1687\n","Epoch 81/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1672\n","Epoch 81: val_loss improved from 0.16867 to 0.16725, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1672 - val_loss: 0.1672\n","Epoch 82/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1657\n","Epoch 82: val_loss improved from 0.16725 to 0.16584, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1657 - val_loss: 0.1658\n","Epoch 83/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1641\n","Epoch 83: val_loss improved from 0.16584 to 0.16446, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1641 - val_loss: 0.1645\n","Epoch 84/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1626\n","Epoch 84: val_loss improved from 0.16446 to 0.16309, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1626 - val_loss: 0.1631\n","Epoch 85/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1610\n","Epoch 85: val_loss improved from 0.16309 to 0.16173, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1610 - val_loss: 0.1617\n","Epoch 86/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1595\n","Epoch 86: val_loss improved from 0.16173 to 0.16039, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.1595 - val_loss: 0.1604\n","Epoch 87/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1580\n","Epoch 87: val_loss improved from 0.16039 to 0.15906, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1580 - val_loss: 0.1591\n","Epoch 88/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1566\n","Epoch 88: val_loss improved from 0.15906 to 0.15775, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1566 - val_loss: 0.1578\n","Epoch 89/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1551\n","Epoch 89: val_loss improved from 0.15775 to 0.15646, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.1551 - val_loss: 0.1565\n","Epoch 90/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1537\n","Epoch 90: val_loss improved from 0.15646 to 0.15518, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 137ms/step - loss: 0.1537 - val_loss: 0.1552\n","Epoch 91/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1522\n","Epoch 91: val_loss improved from 0.15518 to 0.15391, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.1522 - val_loss: 0.1539\n","Epoch 92/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1508\n","Epoch 92: val_loss improved from 0.15391 to 0.15266, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.1508 - val_loss: 0.1527\n","Epoch 93/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1494\n","Epoch 93: val_loss improved from 0.15266 to 0.15142, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.1494 - val_loss: 0.1514\n","Epoch 94/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1481\n","Epoch 94: val_loss improved from 0.15142 to 0.15020, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.1481 - val_loss: 0.1502\n","Epoch 95/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1467\n","Epoch 95: val_loss improved from 0.15020 to 0.14899, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 132ms/step - loss: 0.1467 - val_loss: 0.1490\n","Epoch 96/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1453\n","Epoch 96: val_loss improved from 0.14899 to 0.14779, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.1453 - val_loss: 0.1478\n","Epoch 97/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1440\n","Epoch 97: val_loss improved from 0.14779 to 0.14661, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.1440 - val_loss: 0.1466\n","Epoch 98/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1427\n","Epoch 98: val_loss improved from 0.14661 to 0.14544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 128ms/step - loss: 0.1427 - val_loss: 0.1454\n","Epoch 99/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1414\n","Epoch 99: val_loss improved from 0.14544 to 0.14429, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.1414 - val_loss: 0.1443\n","Epoch 100/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1401\n","Epoch 100: val_loss improved from 0.14429 to 0.14315, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.1401 - val_loss: 0.1431\n","Epoch 101/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1388\n","Epoch 101: val_loss improved from 0.14315 to 0.14202, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.1388 - val_loss: 0.1420\n","Epoch 102/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1375\n","Epoch 102: val_loss improved from 0.14202 to 0.14090, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.1375 - val_loss: 0.1409\n","Epoch 103/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1363\n","Epoch 103: val_loss improved from 0.14090 to 0.13980, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.1363 - val_loss: 0.1398\n","Epoch 104/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1350\n","Epoch 104: val_loss improved from 0.13980 to 0.13871, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 123ms/step - loss: 0.1350 - val_loss: 0.1387\n","Epoch 105/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1338\n","Epoch 105: val_loss improved from 0.13871 to 0.13763, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.1338 - val_loss: 0.1376\n","Epoch 106/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1326\n","Epoch 106: val_loss improved from 0.13763 to 0.13656, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 129ms/step - loss: 0.1326 - val_loss: 0.1366\n","Epoch 107/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1314\n","Epoch 107: val_loss improved from 0.13656 to 0.13551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 130ms/step - loss: 0.1314 - val_loss: 0.1355\n","Epoch 108/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1302\n","Epoch 108: val_loss improved from 0.13551 to 0.13447, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.1302 - val_loss: 0.1345\n","Epoch 109/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1290\n","Epoch 109: val_loss improved from 0.13447 to 0.13344, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.1290 - val_loss: 0.1334\n","Epoch 110/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1279\n","Epoch 110: val_loss improved from 0.13344 to 0.13242, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.1279 - val_loss: 0.1324\n","Epoch 111/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1267\n","Epoch 111: val_loss improved from 0.13242 to 0.13142, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.1267 - val_loss: 0.1314\n","Epoch 112/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1256\n","Epoch 112: val_loss improved from 0.13142 to 0.13042, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.1256 - val_loss: 0.1304\n","Epoch 113/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1245\n","Epoch 113: val_loss improved from 0.13042 to 0.12944, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.1245 - val_loss: 0.1294\n","Epoch 114/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1233\n","Epoch 114: val_loss improved from 0.12944 to 0.12847, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.1233 - val_loss: 0.1285\n","Epoch 115/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1222\n","Epoch 115: val_loss improved from 0.12847 to 0.12751, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1222 - val_loss: 0.1275\n","Epoch 116/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1212\n","Epoch 116: val_loss improved from 0.12751 to 0.12656, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1212 - val_loss: 0.1266\n","Epoch 117/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1201\n","Epoch 117: val_loss improved from 0.12656 to 0.12562, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1201 - val_loss: 0.1256\n","Epoch 118/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1190\n","Epoch 118: val_loss improved from 0.12562 to 0.12470, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.1190 - val_loss: 0.1247\n","Epoch 119/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1180\n","Epoch 119: val_loss improved from 0.12470 to 0.12378, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1180 - val_loss: 0.1238\n","Epoch 120/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1169\n","Epoch 120: val_loss improved from 0.12378 to 0.12288, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.1169 - val_loss: 0.1229\n","Epoch 121/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1159\n","Epoch 121: val_loss improved from 0.12288 to 0.12198, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1159 - val_loss: 0.1220\n","Epoch 122/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1149\n","Epoch 122: val_loss improved from 0.12198 to 0.12110, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1149 - val_loss: 0.1211\n","Epoch 123/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1139\n","Epoch 123: val_loss improved from 0.12110 to 0.12023, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1139 - val_loss: 0.1202\n","Epoch 124/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1129\n","Epoch 124: val_loss improved from 0.12023 to 0.11936, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1129 - val_loss: 0.1194\n","Epoch 125/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1119\n","Epoch 125: val_loss improved from 0.11936 to 0.11851, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1119 - val_loss: 0.1185\n","Epoch 126/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1109\n","Epoch 126: val_loss improved from 0.11851 to 0.11766, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1109 - val_loss: 0.1177\n","Epoch 127/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1099\n","Epoch 127: val_loss improved from 0.11766 to 0.11683, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1099 - val_loss: 0.1168\n","Epoch 128/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1090\n","Epoch 128: val_loss improved from 0.11683 to 0.11601, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1090 - val_loss: 0.1160\n","Epoch 129/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1080\n","Epoch 129: val_loss improved from 0.11601 to 0.11519, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1080 - val_loss: 0.1152\n","Epoch 130/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1071\n","Epoch 130: val_loss improved from 0.11519 to 0.11439, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1071 - val_loss: 0.1144\n","Epoch 131/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1062\n","Epoch 131: val_loss improved from 0.11439 to 0.11359, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.1062 - val_loss: 0.1136\n","Epoch 132/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1053\n","Epoch 132: val_loss improved from 0.11359 to 0.11281, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1053 - val_loss: 0.1128\n","Epoch 133/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1044\n","Epoch 133: val_loss improved from 0.11281 to 0.11203, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1044 - val_loss: 0.1120\n","Epoch 134/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1035\n","Epoch 134: val_loss improved from 0.11203 to 0.11126, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1035 - val_loss: 0.1113\n","Epoch 135/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1026\n","Epoch 135: val_loss improved from 0.11126 to 0.11050, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1026 - val_loss: 0.1105\n","Epoch 136/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1017\n","Epoch 136: val_loss improved from 0.11050 to 0.10975, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1017 - val_loss: 0.1098\n","Epoch 137/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1008\n","Epoch 137: val_loss improved from 0.10975 to 0.10901, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1008 - val_loss: 0.1090\n","Epoch 138/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1000\n","Epoch 138: val_loss improved from 0.10901 to 0.10828, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1000 - val_loss: 0.1083\n","Epoch 139/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0991\n","Epoch 139: val_loss improved from 0.10828 to 0.10755, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0991 - val_loss: 0.1076\n","Epoch 140/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0983\n","Epoch 140: val_loss improved from 0.10755 to 0.10684, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0983 - val_loss: 0.1068\n","Epoch 141/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0975\n","Epoch 141: val_loss improved from 0.10684 to 0.10613, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0975 - val_loss: 0.1061\n","Epoch 142/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0966\n","Epoch 142: val_loss improved from 0.10613 to 0.10542, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0966 - val_loss: 0.1054\n","Epoch 143/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0958\n","Epoch 143: val_loss improved from 0.10542 to 0.10473, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0958 - val_loss: 0.1047\n","Epoch 144/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0950\n","Epoch 144: val_loss improved from 0.10473 to 0.10405, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0950 - val_loss: 0.1040\n","Epoch 145/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0942\n","Epoch 145: val_loss improved from 0.10405 to 0.10337, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0942 - val_loss: 0.1034\n","Epoch 146/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0934\n","Epoch 146: val_loss improved from 0.10337 to 0.10270, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0934 - val_loss: 0.1027\n","Epoch 147/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0926\n","Epoch 147: val_loss improved from 0.10270 to 0.10204, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0926 - val_loss: 0.1020\n","Epoch 148/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0919\n","Epoch 148: val_loss improved from 0.10204 to 0.10139, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0919 - val_loss: 0.1014\n","Epoch 149/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0911\n","Epoch 149: val_loss improved from 0.10139 to 0.10075, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0911 - val_loss: 0.1007\n","Epoch 150/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0904\n","Epoch 150: val_loss improved from 0.10075 to 0.10011, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0904 - val_loss: 0.1001\n","Epoch 151/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0896\n","Epoch 151: val_loss improved from 0.10011 to 0.09948, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0896 - val_loss: 0.0995\n","Epoch 152/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0889\n","Epoch 152: val_loss improved from 0.09948 to 0.09886, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0889 - val_loss: 0.0989\n","Epoch 153/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0882\n","Epoch 153: val_loss improved from 0.09886 to 0.09825, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0882 - val_loss: 0.0983\n","Epoch 154/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0874\n","Epoch 154: val_loss improved from 0.09825 to 0.09765, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0874 - val_loss: 0.0976\n","Epoch 155/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0867\n","Epoch 155: val_loss improved from 0.09765 to 0.09705, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0867 - val_loss: 0.0970\n","Epoch 156/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0860\n","Epoch 156: val_loss improved from 0.09705 to 0.09646, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0860 - val_loss: 0.0965\n","Epoch 157/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0853\n","Epoch 157: val_loss improved from 0.09646 to 0.09587, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0853 - val_loss: 0.0959\n","Epoch 158/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0846\n","Epoch 158: val_loss improved from 0.09587 to 0.09530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.0846 - val_loss: 0.0953\n","Epoch 159/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0840\n","Epoch 159: val_loss improved from 0.09530 to 0.09473, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0840 - val_loss: 0.0947\n","Epoch 160/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0833\n","Epoch 160: val_loss improved from 0.09473 to 0.09417, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0833 - val_loss: 0.0942\n","Epoch 161/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0826\n","Epoch 161: val_loss improved from 0.09417 to 0.09361, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0826 - val_loss: 0.0936\n","Epoch 162/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0820\n","Epoch 162: val_loss improved from 0.09361 to 0.09306, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0820 - val_loss: 0.0931\n","Epoch 163/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0813\n","Epoch 163: val_loss improved from 0.09306 to 0.09252, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0813 - val_loss: 0.0925\n","Epoch 164/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0807\n","Epoch 164: val_loss improved from 0.09252 to 0.09199, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0807 - val_loss: 0.0920\n","Epoch 165/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0800\n","Epoch 165: val_loss improved from 0.09199 to 0.09146, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0800 - val_loss: 0.0915\n","Epoch 166/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0794\n","Epoch 166: val_loss improved from 0.09146 to 0.09094, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0794 - val_loss: 0.0909\n","Epoch 167/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0788\n","Epoch 167: val_loss improved from 0.09094 to 0.09042, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0788 - val_loss: 0.0904\n","Epoch 168/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0782\n","Epoch 168: val_loss improved from 0.09042 to 0.08992, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0782 - val_loss: 0.0899\n","Epoch 169/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0776\n","Epoch 169: val_loss improved from 0.08992 to 0.08941, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0776 - val_loss: 0.0894\n","Epoch 170/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0770\n","Epoch 170: val_loss improved from 0.08941 to 0.08892, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0770 - val_loss: 0.0889\n","Epoch 171/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0764\n","Epoch 171: val_loss improved from 0.08892 to 0.08843, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0764 - val_loss: 0.0884\n","Epoch 172/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0758\n","Epoch 172: val_loss improved from 0.08843 to 0.08795, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0758 - val_loss: 0.0879\n","Epoch 173/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0752\n","Epoch 173: val_loss improved from 0.08795 to 0.08747, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0752 - val_loss: 0.0875\n","Epoch 174/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0746\n","Epoch 174: val_loss improved from 0.08747 to 0.08700, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0746 - val_loss: 0.0870\n","Epoch 175/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0741\n","Epoch 175: val_loss improved from 0.08700 to 0.08653, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0741 - val_loss: 0.0865\n","Epoch 176/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0735\n","Epoch 176: val_loss improved from 0.08653 to 0.08607, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0735 - val_loss: 0.0861\n","Epoch 177/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0729\n","Epoch 177: val_loss improved from 0.08607 to 0.08562, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0729 - val_loss: 0.0856\n","Epoch 178/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0724\n","Epoch 178: val_loss improved from 0.08562 to 0.08517, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0724 - val_loss: 0.0852\n","Epoch 179/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0719\n","Epoch 179: val_loss improved from 0.08517 to 0.08473, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0719 - val_loss: 0.0847\n","Epoch 180/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0713\n","Epoch 180: val_loss improved from 0.08473 to 0.08429, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0713 - val_loss: 0.0843\n","Epoch 181/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0708\n","Epoch 181: val_loss improved from 0.08429 to 0.08386, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0708 - val_loss: 0.0839\n","Epoch 182/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0703\n","Epoch 182: val_loss improved from 0.08386 to 0.08344, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0703 - val_loss: 0.0834\n","Epoch 183/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0698\n","Epoch 183: val_loss improved from 0.08344 to 0.08302, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0698 - val_loss: 0.0830\n","Epoch 184/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0692\n","Epoch 184: val_loss improved from 0.08302 to 0.08260, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0692 - val_loss: 0.0826\n","Epoch 185/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0687\n","Epoch 185: val_loss improved from 0.08260 to 0.08220, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0687 - val_loss: 0.0822\n","Epoch 186/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0682\n","Epoch 186: val_loss improved from 0.08220 to 0.08179, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0682 - val_loss: 0.0818\n","Epoch 187/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0678\n","Epoch 187: val_loss improved from 0.08179 to 0.08139, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0678 - val_loss: 0.0814\n","Epoch 188/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0673\n","Epoch 188: val_loss improved from 0.08139 to 0.08100, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0673 - val_loss: 0.0810\n","Epoch 189/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0668\n","Epoch 189: val_loss improved from 0.08100 to 0.08061, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0668 - val_loss: 0.0806\n","Epoch 190/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0663\n","Epoch 190: val_loss improved from 0.08061 to 0.08023, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0663 - val_loss: 0.0802\n","Epoch 191/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0658\n","Epoch 191: val_loss improved from 0.08023 to 0.07985, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0658 - val_loss: 0.0799\n","Epoch 192/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0654\n","Epoch 192: val_loss improved from 0.07985 to 0.07948, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0654 - val_loss: 0.0795\n","Epoch 193/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0649\n","Epoch 193: val_loss improved from 0.07948 to 0.07911, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0649 - val_loss: 0.0791\n","Epoch 194/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0645\n","Epoch 194: val_loss improved from 0.07911 to 0.07874, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0645 - val_loss: 0.0787\n","Epoch 195/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0640\n","Epoch 195: val_loss improved from 0.07874 to 0.07839, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0640 - val_loss: 0.0784\n","Epoch 196/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0636\n","Epoch 196: val_loss improved from 0.07839 to 0.07803, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0636 - val_loss: 0.0780\n","Epoch 197/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0631\n","Epoch 197: val_loss improved from 0.07803 to 0.07768, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0631 - val_loss: 0.0777\n","Epoch 198/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0627\n","Epoch 198: val_loss improved from 0.07768 to 0.07734, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0627 - val_loss: 0.0773\n","Epoch 199/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0623\n","Epoch 199: val_loss improved from 0.07734 to 0.07700, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0623 - val_loss: 0.0770\n","Epoch 200/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0618\n","Epoch 200: val_loss improved from 0.07700 to 0.07666, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0618 - val_loss: 0.0767\n","Epoch 201/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0614\n","Epoch 201: val_loss improved from 0.07666 to 0.07633, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0614 - val_loss: 0.0763\n","Epoch 202/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0610\n","Epoch 202: val_loss improved from 0.07633 to 0.07600, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0610 - val_loss: 0.0760\n","Epoch 203/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0606\n","Epoch 203: val_loss improved from 0.07600 to 0.07568, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0606 - val_loss: 0.0757\n","Epoch 204/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0602\n","Epoch 204: val_loss improved from 0.07568 to 0.07536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0602 - val_loss: 0.0754\n","Epoch 205/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0598\n","Epoch 205: val_loss improved from 0.07536 to 0.07505, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0598 - val_loss: 0.0750\n","Epoch 206/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0594\n","Epoch 206: val_loss improved from 0.07505 to 0.07474, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0594 - val_loss: 0.0747\n","Epoch 207/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0590\n","Epoch 207: val_loss improved from 0.07474 to 0.07443, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0590 - val_loss: 0.0744\n","Epoch 208/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0586\n","Epoch 208: val_loss improved from 0.07443 to 0.07413, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0586 - val_loss: 0.0741\n","Epoch 209/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0582\n","Epoch 209: val_loss improved from 0.07413 to 0.07383, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0582 - val_loss: 0.0738\n","Epoch 210/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0579\n","Epoch 210: val_loss improved from 0.07383 to 0.07354, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0579 - val_loss: 0.0735\n","Epoch 211/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0575\n","Epoch 211: val_loss improved from 0.07354 to 0.07325, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0575 - val_loss: 0.0733\n","Epoch 212/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0571\n","Epoch 212: val_loss improved from 0.07325 to 0.07297, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0571 - val_loss: 0.0730\n","Epoch 213/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0568\n","Epoch 213: val_loss improved from 0.07297 to 0.07268, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0568 - val_loss: 0.0727\n","Epoch 214/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0564\n","Epoch 214: val_loss improved from 0.07268 to 0.07241, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0564 - val_loss: 0.0724\n","Epoch 215/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0561\n","Epoch 215: val_loss improved from 0.07241 to 0.07213, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0561 - val_loss: 0.0721\n","Epoch 216/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0557\n","Epoch 216: val_loss improved from 0.07213 to 0.07186, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0557 - val_loss: 0.0719\n","Epoch 217/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0554\n","Epoch 217: val_loss improved from 0.07186 to 0.07160, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0554 - val_loss: 0.0716\n","Epoch 218/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0550\n","Epoch 218: val_loss improved from 0.07160 to 0.07133, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0550 - val_loss: 0.0713\n","Epoch 219/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0547\n","Epoch 219: val_loss improved from 0.07133 to 0.07107, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0547 - val_loss: 0.0711\n","Epoch 220/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0543\n","Epoch 220: val_loss improved from 0.07107 to 0.07082, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0543 - val_loss: 0.0708\n","Epoch 221/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0540\n","Epoch 221: val_loss improved from 0.07082 to 0.07056, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0540 - val_loss: 0.0706\n","Epoch 222/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0537\n","Epoch 222: val_loss improved from 0.07056 to 0.07032, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0537 - val_loss: 0.0703\n","Epoch 223/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0534\n","Epoch 223: val_loss improved from 0.07032 to 0.07007, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0534 - val_loss: 0.0701\n","Epoch 224/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0530\n","Epoch 224: val_loss improved from 0.07007 to 0.06983, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0530 - val_loss: 0.0698\n","Epoch 225/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0527\n","Epoch 225: val_loss improved from 0.06983 to 0.06959, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0527 - val_loss: 0.0696\n","Epoch 226/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0524\n","Epoch 226: val_loss improved from 0.06959 to 0.06935, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0524 - val_loss: 0.0694\n","Epoch 227/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0521\n","Epoch 227: val_loss improved from 0.06935 to 0.06912, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0521 - val_loss: 0.0691\n","Epoch 228/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0518\n","Epoch 228: val_loss improved from 0.06912 to 0.06889, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0518 - val_loss: 0.0689\n","Epoch 229/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0515\n","Epoch 229: val_loss improved from 0.06889 to 0.06867, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0515 - val_loss: 0.0687\n","Epoch 230/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0512\n","Epoch 230: val_loss improved from 0.06867 to 0.06845, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0512 - val_loss: 0.0684\n","Epoch 231/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0509\n","Epoch 231: val_loss improved from 0.06845 to 0.06823, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0509 - val_loss: 0.0682\n","Epoch 232/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0506\n","Epoch 232: val_loss improved from 0.06823 to 0.06801, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0506 - val_loss: 0.0680\n","Epoch 233/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0503\n","Epoch 233: val_loss improved from 0.06801 to 0.06780, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0503 - val_loss: 0.0678\n","Epoch 234/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0501\n","Epoch 234: val_loss improved from 0.06780 to 0.06759, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0501 - val_loss: 0.0676\n","Epoch 235/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0498\n","Epoch 235: val_loss improved from 0.06759 to 0.06738, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.0498 - val_loss: 0.0674\n","Epoch 236/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0495\n","Epoch 236: val_loss improved from 0.06738 to 0.06718, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 127ms/step - loss: 0.0495 - val_loss: 0.0672\n","Epoch 237/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0492\n","Epoch 237: val_loss improved from 0.06718 to 0.06697, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0492 - val_loss: 0.0670\n","Epoch 238/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0490\n","Epoch 238: val_loss improved from 0.06697 to 0.06678, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0490 - val_loss: 0.0668\n","Epoch 239/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0487\n","Epoch 239: val_loss improved from 0.06678 to 0.06658, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0487 - val_loss: 0.0666\n","Epoch 240/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0484\n","Epoch 240: val_loss improved from 0.06658 to 0.06639, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 128ms/step - loss: 0.0484 - val_loss: 0.0664\n","Epoch 241/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0482\n","Epoch 241: val_loss improved from 0.06639 to 0.06620, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0482 - val_loss: 0.0662\n","Epoch 242/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0479\n","Epoch 242: val_loss improved from 0.06620 to 0.06601, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0479 - val_loss: 0.0660\n","Epoch 243/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0477\n","Epoch 243: val_loss improved from 0.06601 to 0.06583, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 119ms/step - loss: 0.0477 - val_loss: 0.0658\n","Epoch 244/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0474\n","Epoch 244: val_loss improved from 0.06583 to 0.06564, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.0474 - val_loss: 0.0656\n","Epoch 245/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0472\n","Epoch 245: val_loss improved from 0.06564 to 0.06546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 141ms/step - loss: 0.0472 - val_loss: 0.0655\n","Epoch 246/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0469\n","Epoch 246: val_loss improved from 0.06546 to 0.06529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0469 - val_loss: 0.0653\n","Epoch 247/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0467\n","Epoch 247: val_loss improved from 0.06529 to 0.06511, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 136ms/step - loss: 0.0467 - val_loss: 0.0651\n","Epoch 248/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0465\n","Epoch 248: val_loss improved from 0.06511 to 0.06494, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.0465 - val_loss: 0.0649\n","Epoch 249/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0462\n","Epoch 249: val_loss improved from 0.06494 to 0.06477, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0462 - val_loss: 0.0648\n","Epoch 250/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0460\n","Epoch 250: val_loss improved from 0.06477 to 0.06461, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0460 - val_loss: 0.0646\n","Epoch 251/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0458\n","Epoch 251: val_loss improved from 0.06461 to 0.06444, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0458 - val_loss: 0.0644\n","Epoch 252/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0455\n","Epoch 252: val_loss improved from 0.06444 to 0.06428, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0455 - val_loss: 0.0643\n","Epoch 253/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0453\n","Epoch 253: val_loss improved from 0.06428 to 0.06412, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 140ms/step - loss: 0.0453 - val_loss: 0.0641\n","Epoch 254/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0451\n","Epoch 254: val_loss improved from 0.06412 to 0.06396, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0451 - val_loss: 0.0640\n","Epoch 255/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0449\n","Epoch 255: val_loss improved from 0.06396 to 0.06381, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0449 - val_loss: 0.0638\n","Epoch 256/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0447\n","Epoch 256: val_loss improved from 0.06381 to 0.06366, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0447 - val_loss: 0.0637\n","Epoch 257/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0444\n","Epoch 257: val_loss improved from 0.06366 to 0.06351, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0444 - val_loss: 0.0635\n","Epoch 258/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0442\n","Epoch 258: val_loss improved from 0.06351 to 0.06336, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0442 - val_loss: 0.0634\n","Epoch 259/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0440\n","Epoch 259: val_loss improved from 0.06336 to 0.06321, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 133ms/step - loss: 0.0440 - val_loss: 0.0632\n","Epoch 260/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0438\n","Epoch 260: val_loss improved from 0.06321 to 0.06307, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0438 - val_loss: 0.0631\n","Epoch 261/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0436\n","Epoch 261: val_loss improved from 0.06307 to 0.06293, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0436 - val_loss: 0.0629\n","Epoch 262/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0434\n","Epoch 262: val_loss improved from 0.06293 to 0.06279, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0434 - val_loss: 0.0628\n","Epoch 263/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0432\n","Epoch 263: val_loss improved from 0.06279 to 0.06265, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0432 - val_loss: 0.0627\n","Epoch 264/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0430\n","Epoch 264: val_loss improved from 0.06265 to 0.06252, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.0430 - val_loss: 0.0625\n","Epoch 265/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0428\n","Epoch 265: val_loss improved from 0.06252 to 0.06238, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0428 - val_loss: 0.0624\n","Epoch 266/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0427\n","Epoch 266: val_loss improved from 0.06238 to 0.06225, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0427 - val_loss: 0.0623\n","Epoch 267/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0425\n","Epoch 267: val_loss improved from 0.06225 to 0.06212, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0425 - val_loss: 0.0621\n","Epoch 268/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0423\n","Epoch 268: val_loss improved from 0.06212 to 0.06200, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0423 - val_loss: 0.0620\n","Epoch 269/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0421\n","Epoch 269: val_loss improved from 0.06200 to 0.06187, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0421 - val_loss: 0.0619\n","Epoch 270/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0419\n","Epoch 270: val_loss improved from 0.06187 to 0.06175, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0419 - val_loss: 0.0617\n","Epoch 271/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0417\n","Epoch 271: val_loss improved from 0.06175 to 0.06163, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0417 - val_loss: 0.0616\n","Epoch 272/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0416\n","Epoch 272: val_loss improved from 0.06163 to 0.06151, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0416 - val_loss: 0.0615\n","Epoch 273/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0414\n","Epoch 273: val_loss improved from 0.06151 to 0.06139, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0414 - val_loss: 0.0614\n","Epoch 274/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0412\n","Epoch 274: val_loss improved from 0.06139 to 0.06127, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0412 - val_loss: 0.0613\n","Epoch 275/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0410\n","Epoch 275: val_loss improved from 0.06127 to 0.06116, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0410 - val_loss: 0.0612\n","Epoch 276/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0409\n","Epoch 276: val_loss improved from 0.06116 to 0.06105, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0409 - val_loss: 0.0610\n","Epoch 277/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0407\n","Epoch 277: val_loss improved from 0.06105 to 0.06094, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0407 - val_loss: 0.0609\n","Epoch 278/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0405\n","Epoch 278: val_loss improved from 0.06094 to 0.06083, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0405 - val_loss: 0.0608\n","Epoch 279/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0404\n","Epoch 279: val_loss improved from 0.06083 to 0.06072, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0404 - val_loss: 0.0607\n","Epoch 280/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0402\n","Epoch 280: val_loss improved from 0.06072 to 0.06062, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0402 - val_loss: 0.0606\n","Epoch 281/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0401\n","Epoch 281: val_loss improved from 0.06062 to 0.06051, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0401 - val_loss: 0.0605\n","Epoch 282/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0399\n","Epoch 282: val_loss improved from 0.06051 to 0.06041, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0399 - val_loss: 0.0604\n","Epoch 283/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0398\n","Epoch 283: val_loss improved from 0.06041 to 0.06031, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0398 - val_loss: 0.0603\n","Epoch 284/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0396\n","Epoch 284: val_loss improved from 0.06031 to 0.06021, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0396 - val_loss: 0.0602\n","Epoch 285/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0395\n","Epoch 285: val_loss improved from 0.06021 to 0.06012, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0395 - val_loss: 0.0601\n","Epoch 286/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0393\n","Epoch 286: val_loss improved from 0.06012 to 0.06002, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0393 - val_loss: 0.0600\n","Epoch 287/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0392\n","Epoch 287: val_loss improved from 0.06002 to 0.05993, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0392 - val_loss: 0.0599\n","Epoch 288/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0390\n","Epoch 288: val_loss improved from 0.05993 to 0.05983, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0390 - val_loss: 0.0598\n","Epoch 289/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0389\n","Epoch 289: val_loss improved from 0.05983 to 0.05974, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0389 - val_loss: 0.0597\n","Epoch 290/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0387\n","Epoch 290: val_loss improved from 0.05974 to 0.05965, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0387 - val_loss: 0.0597\n","Epoch 291/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0386\n","Epoch 291: val_loss improved from 0.05965 to 0.05956, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0386 - val_loss: 0.0596\n","Epoch 292/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0385\n","Epoch 292: val_loss improved from 0.05956 to 0.05948, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0385 - val_loss: 0.0595\n","Epoch 293/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0383\n","Epoch 293: val_loss improved from 0.05948 to 0.05939, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0383 - val_loss: 0.0594\n","Epoch 294/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0382\n","Epoch 294: val_loss improved from 0.05939 to 0.05931, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0382 - val_loss: 0.0593\n","Epoch 295/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0381\n","Epoch 295: val_loss improved from 0.05931 to 0.05923, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0381 - val_loss: 0.0592\n","Epoch 296/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0379\n","Epoch 296: val_loss improved from 0.05923 to 0.05915, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0379 - val_loss: 0.0591\n","Epoch 297/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0378\n","Epoch 297: val_loss improved from 0.05915 to 0.05907, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0378 - val_loss: 0.0591\n","Epoch 298/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0377\n","Epoch 298: val_loss improved from 0.05907 to 0.05899, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0377 - val_loss: 0.0590\n","Epoch 299/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0375\n","Epoch 299: val_loss improved from 0.05899 to 0.05891, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0375 - val_loss: 0.0589\n","Epoch 300/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0374\n","Epoch 300: val_loss improved from 0.05891 to 0.05883, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0374 - val_loss: 0.0588\n","Epoch 301/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0373\n","Epoch 301: val_loss improved from 0.05883 to 0.05876, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0373 - val_loss: 0.0588\n","Epoch 302/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0372\n","Epoch 302: val_loss improved from 0.05876 to 0.05869, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0372 - val_loss: 0.0587\n","Epoch 303/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0371\n","Epoch 303: val_loss improved from 0.05869 to 0.05861, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0371 - val_loss: 0.0586\n","Epoch 304/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0369\n","Epoch 304: val_loss improved from 0.05861 to 0.05854, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0369 - val_loss: 0.0585\n","Epoch 305/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0368\n","Epoch 305: val_loss improved from 0.05854 to 0.05847, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0368 - val_loss: 0.0585\n","Epoch 306/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0367\n","Epoch 306: val_loss improved from 0.05847 to 0.05841, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0367 - val_loss: 0.0584\n","Epoch 307/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0366\n","Epoch 307: val_loss improved from 0.05841 to 0.05834, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0366 - val_loss: 0.0583\n","Epoch 308/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0365\n","Epoch 308: val_loss improved from 0.05834 to 0.05827, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0365 - val_loss: 0.0583\n","Epoch 309/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0364\n","Epoch 309: val_loss improved from 0.05827 to 0.05821, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0364 - val_loss: 0.0582\n","Epoch 310/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0363\n","Epoch 310: val_loss improved from 0.05821 to 0.05814, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0363 - val_loss: 0.0581\n","Epoch 311/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0362\n","Epoch 311: val_loss improved from 0.05814 to 0.05808, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0362 - val_loss: 0.0581\n","Epoch 312/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 312: val_loss improved from 0.05808 to 0.05802, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0361 - val_loss: 0.0580\n","Epoch 313/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0360\n","Epoch 313: val_loss improved from 0.05802 to 0.05796, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0360 - val_loss: 0.0580\n","Epoch 314/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0359\n","Epoch 314: val_loss improved from 0.05796 to 0.05790, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0359 - val_loss: 0.0579\n","Epoch 315/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0357\n","Epoch 315: val_loss improved from 0.05790 to 0.05784, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0357 - val_loss: 0.0578\n","Epoch 316/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 316: val_loss improved from 0.05784 to 0.05778, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0356 - val_loss: 0.0578\n","Epoch 317/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0355\n","Epoch 317: val_loss improved from 0.05778 to 0.05773, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0355 - val_loss: 0.0577\n","Epoch 318/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0355\n","Epoch 318: val_loss improved from 0.05773 to 0.05767, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0355 - val_loss: 0.0577\n","Epoch 319/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 319: val_loss improved from 0.05767 to 0.05762, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0354 - val_loss: 0.0576\n","Epoch 320/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0353\n","Epoch 320: val_loss improved from 0.05762 to 0.05756, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0353 - val_loss: 0.0576\n","Epoch 321/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0352\n","Epoch 321: val_loss improved from 0.05756 to 0.05751, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0352 - val_loss: 0.0575\n","Epoch 322/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0351\n","Epoch 322: val_loss improved from 0.05751 to 0.05746, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0351 - val_loss: 0.0575\n","Epoch 323/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0350\n","Epoch 323: val_loss improved from 0.05746 to 0.05741, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0350 - val_loss: 0.0574\n","Epoch 324/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0349\n","Epoch 324: val_loss improved from 0.05741 to 0.05736, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0349 - val_loss: 0.0574\n","Epoch 325/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0348\n","Epoch 325: val_loss improved from 0.05736 to 0.05731, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0348 - val_loss: 0.0573\n","Epoch 326/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 326: val_loss improved from 0.05731 to 0.05726, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0347 - val_loss: 0.0573\n","Epoch 327/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 327: val_loss improved from 0.05726 to 0.05722, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0346 - val_loss: 0.0572\n","Epoch 328/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0345\n","Epoch 328: val_loss improved from 0.05722 to 0.05717, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 121ms/step - loss: 0.0345 - val_loss: 0.0572\n","Epoch 329/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0345\n","Epoch 329: val_loss improved from 0.05717 to 0.05712, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0345 - val_loss: 0.0571\n","Epoch 330/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 330: val_loss improved from 0.05712 to 0.05708, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0344 - val_loss: 0.0571\n","Epoch 331/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 331: val_loss improved from 0.05708 to 0.05704, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0343 - val_loss: 0.0570\n","Epoch 332/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 332: val_loss improved from 0.05704 to 0.05699, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0342 - val_loss: 0.0570\n","Epoch 333/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 333: val_loss improved from 0.05699 to 0.05695, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0341 - val_loss: 0.0570\n","Epoch 334/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 334: val_loss improved from 0.05695 to 0.05691, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0340 - val_loss: 0.0569\n","Epoch 335/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 335: val_loss improved from 0.05691 to 0.05687, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0340 - val_loss: 0.0569\n","Epoch 336/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0339\n","Epoch 336: val_loss improved from 0.05687 to 0.05683, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0339 - val_loss: 0.0568\n","Epoch 337/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 337: val_loss improved from 0.05683 to 0.05679, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0338 - val_loss: 0.0568\n","Epoch 338/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 338: val_loss improved from 0.05679 to 0.05675, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0337 - val_loss: 0.0568\n","Epoch 339/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 339: val_loss improved from 0.05675 to 0.05672, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0337 - val_loss: 0.0567\n","Epoch 340/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 340: val_loss improved from 0.05672 to 0.05668, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0336 - val_loss: 0.0567\n","Epoch 341/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 341: val_loss improved from 0.05668 to 0.05664, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0335 - val_loss: 0.0566\n","Epoch 342/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 342: val_loss improved from 0.05664 to 0.05661, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0334 - val_loss: 0.0566\n","Epoch 343/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 343: val_loss improved from 0.05661 to 0.05657, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0334 - val_loss: 0.0566\n","Epoch 344/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 344: val_loss improved from 0.05657 to 0.05654, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0333 - val_loss: 0.0565\n","Epoch 345/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 345: val_loss improved from 0.05654 to 0.05651, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0332 - val_loss: 0.0565\n","Epoch 346/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 346: val_loss improved from 0.05651 to 0.05648, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0332 - val_loss: 0.0565\n","Epoch 347/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0331\n","Epoch 347: val_loss improved from 0.05648 to 0.05644, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0331 - val_loss: 0.0564\n","Epoch 348/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 348: val_loss improved from 0.05644 to 0.05641, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0330 - val_loss: 0.0564\n","Epoch 349/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 349: val_loss improved from 0.05641 to 0.05638, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0330 - val_loss: 0.0564\n","Epoch 350/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 350: val_loss improved from 0.05638 to 0.05635, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0329 - val_loss: 0.0564\n","Epoch 351/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 351: val_loss improved from 0.05635 to 0.05632, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0328 - val_loss: 0.0563\n","Epoch 352/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 352: val_loss improved from 0.05632 to 0.05629, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0328 - val_loss: 0.0563\n","Epoch 353/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 353: val_loss improved from 0.05629 to 0.05627, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0327 - val_loss: 0.0563\n","Epoch 354/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 354: val_loss improved from 0.05627 to 0.05624, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0327 - val_loss: 0.0562\n","Epoch 355/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 355: val_loss improved from 0.05624 to 0.05621, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0326 - val_loss: 0.0562\n","Epoch 356/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 356: val_loss improved from 0.05621 to 0.05619, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0325 - val_loss: 0.0562\n","Epoch 357/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 357: val_loss improved from 0.05619 to 0.05616, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0325 - val_loss: 0.0562\n","Epoch 358/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 358: val_loss improved from 0.05616 to 0.05614, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0324 - val_loss: 0.0561\n","Epoch 359/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 359: val_loss improved from 0.05614 to 0.05611, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0324 - val_loss: 0.0561\n","Epoch 360/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 360: val_loss improved from 0.05611 to 0.05609, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0323 - val_loss: 0.0561\n","Epoch 361/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 361: val_loss improved from 0.05609 to 0.05606, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0322 - val_loss: 0.0561\n","Epoch 362/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 362: val_loss improved from 0.05606 to 0.05604, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0322 - val_loss: 0.0560\n","Epoch 363/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 363: val_loss improved from 0.05604 to 0.05602, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0321 - val_loss: 0.0560\n","Epoch 364/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 364: val_loss improved from 0.05602 to 0.05600, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0321 - val_loss: 0.0560\n","Epoch 365/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 365: val_loss improved from 0.05600 to 0.05598, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0320 - val_loss: 0.0560\n","Epoch 366/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 366: val_loss improved from 0.05598 to 0.05596, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0320 - val_loss: 0.0560\n","Epoch 367/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 367: val_loss improved from 0.05596 to 0.05593, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0319 - val_loss: 0.0559\n","Epoch 368/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 368: val_loss improved from 0.05593 to 0.05591, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0319 - val_loss: 0.0559\n","Epoch 369/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 369: val_loss improved from 0.05591 to 0.05590, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0318 - val_loss: 0.0559\n","Epoch 370/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 370: val_loss improved from 0.05590 to 0.05588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0318 - val_loss: 0.0559\n","Epoch 371/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 371: val_loss improved from 0.05588 to 0.05586, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0317 - val_loss: 0.0559\n","Epoch 372/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 372: val_loss improved from 0.05586 to 0.05584, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0317 - val_loss: 0.0558\n","Epoch 373/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 373: val_loss improved from 0.05584 to 0.05582, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0316 - val_loss: 0.0558\n","Epoch 374/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 374: val_loss improved from 0.05582 to 0.05581, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0316 - val_loss: 0.0558\n","Epoch 375/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 375: val_loss improved from 0.05581 to 0.05579, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0315 - val_loss: 0.0558\n","Epoch 376/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 376: val_loss improved from 0.05579 to 0.05577, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0315 - val_loss: 0.0558\n","Epoch 377/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 377: val_loss improved from 0.05577 to 0.05576, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0314 - val_loss: 0.0558\n","Epoch 378/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 378: val_loss improved from 0.05576 to 0.05574, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0314 - val_loss: 0.0557\n","Epoch 379/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 379: val_loss improved from 0.05574 to 0.05573, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0314 - val_loss: 0.0557\n","Epoch 380/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 380: val_loss improved from 0.05573 to 0.05571, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0313 - val_loss: 0.0557\n","Epoch 381/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 381: val_loss improved from 0.05571 to 0.05570, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 132ms/step - loss: 0.0313 - val_loss: 0.0557\n","Epoch 382/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 382: val_loss improved from 0.05570 to 0.05568, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0312 - val_loss: 0.0557\n","Epoch 383/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 383: val_loss improved from 0.05568 to 0.05567, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0312 - val_loss: 0.0557\n","Epoch 384/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 384: val_loss improved from 0.05567 to 0.05566, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0311 - val_loss: 0.0557\n","Epoch 385/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 385: val_loss improved from 0.05566 to 0.05564, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.0311 - val_loss: 0.0556\n","Epoch 386/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 386: val_loss improved from 0.05564 to 0.05563, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 128ms/step - loss: 0.0311 - val_loss: 0.0556\n","Epoch 387/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 387: val_loss improved from 0.05563 to 0.05562, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0310 - val_loss: 0.0556\n","Epoch 388/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 388: val_loss improved from 0.05562 to 0.05561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 128ms/step - loss: 0.0310 - val_loss: 0.0556\n","Epoch 389/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 389: val_loss improved from 0.05561 to 0.05559, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0309 - val_loss: 0.0556\n","Epoch 390/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 390: val_loss improved from 0.05559 to 0.05558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0309 - val_loss: 0.0556\n","Epoch 391/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 391: val_loss improved from 0.05558 to 0.05557, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.0309 - val_loss: 0.0556\n","Epoch 392/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 392: val_loss improved from 0.05557 to 0.05556, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.0308 - val_loss: 0.0556\n","Epoch 393/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 393: val_loss improved from 0.05556 to 0.05555, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 132ms/step - loss: 0.0308 - val_loss: 0.0556\n","Epoch 394/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 394: val_loss improved from 0.05555 to 0.05554, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0308 - val_loss: 0.0555\n","Epoch 395/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 395: val_loss improved from 0.05554 to 0.05553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0307 - val_loss: 0.0555\n","Epoch 396/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 396: val_loss improved from 0.05553 to 0.05552, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0307 - val_loss: 0.0555\n","Epoch 397/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 397: val_loss improved from 0.05552 to 0.05551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0307 - val_loss: 0.0555\n","Epoch 398/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 398: val_loss improved from 0.05551 to 0.05551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0306 - val_loss: 0.0555\n","Epoch 399/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 399: val_loss improved from 0.05551 to 0.05550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 120ms/step - loss: 0.0306 - val_loss: 0.0555\n","Epoch 400/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 400: val_loss improved from 0.05550 to 0.05549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0306 - val_loss: 0.0555\n","Epoch 401/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 401: val_loss improved from 0.05549 to 0.05548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0305 - val_loss: 0.0555\n","Epoch 402/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 402: val_loss improved from 0.05548 to 0.05547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 130ms/step - loss: 0.0305 - val_loss: 0.0555\n","Epoch 403/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 403: val_loss improved from 0.05547 to 0.05547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 120ms/step - loss: 0.0305 - val_loss: 0.0555\n","Epoch 404/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 404: val_loss improved from 0.05547 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 120ms/step - loss: 0.0304 - val_loss: 0.0555\n","Epoch 405/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 405: val_loss improved from 0.05546 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0304 - val_loss: 0.0555\n","Epoch 406/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 406: val_loss improved from 0.05545 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 138ms/step - loss: 0.0304 - val_loss: 0.0554\n","Epoch 407/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 407: val_loss improved from 0.05545 to 0.05544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 134ms/step - loss: 0.0303 - val_loss: 0.0554\n","Epoch 408/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 408: val_loss improved from 0.05544 to 0.05543, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.0303 - val_loss: 0.0554\n","Epoch 409/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 409: val_loss improved from 0.05543 to 0.05543, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0303 - val_loss: 0.0554\n","Epoch 410/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 410: val_loss improved from 0.05543 to 0.05542, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0302 - val_loss: 0.0554\n","Epoch 411/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 411: val_loss improved from 0.05542 to 0.05542, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0302 - val_loss: 0.0554\n","Epoch 412/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 412: val_loss improved from 0.05542 to 0.05541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0302 - val_loss: 0.0554\n","Epoch 413/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 413: val_loss improved from 0.05541 to 0.05541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0302 - val_loss: 0.0554\n","Epoch 414/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 414: val_loss improved from 0.05541 to 0.05540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0301 - val_loss: 0.0554\n","Epoch 415/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 415: val_loss improved from 0.05540 to 0.05540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0301 - val_loss: 0.0554\n","Epoch 416/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 416: val_loss improved from 0.05540 to 0.05539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0301 - val_loss: 0.0554\n","Epoch 417/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 417: val_loss improved from 0.05539 to 0.05539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0301 - val_loss: 0.0554\n","Epoch 418/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 418: val_loss improved from 0.05539 to 0.05539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0300 - val_loss: 0.0554\n","Epoch 419/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 419: val_loss improved from 0.05539 to 0.05538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0300 - val_loss: 0.0554\n","Epoch 420/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 420: val_loss improved from 0.05538 to 0.05538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0300 - val_loss: 0.0554\n","Epoch 421/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 421: val_loss improved from 0.05538 to 0.05538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0300 - val_loss: 0.0554\n","Epoch 422/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 422: val_loss improved from 0.05538 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0299 - val_loss: 0.0554\n","Epoch 423/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 423: val_loss improved from 0.05537 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0299 - val_loss: 0.0554\n","Epoch 424/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 424: val_loss improved from 0.05537 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0299 - val_loss: 0.0554\n","Epoch 425/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 425: val_loss improved from 0.05537 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0299 - val_loss: 0.0554\n","Epoch 426/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 426: val_loss improved from 0.05537 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0298 - val_loss: 0.0554\n","Epoch 427/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 427: val_loss improved from 0.05536 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0298 - val_loss: 0.0554\n","Epoch 428/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 428: val_loss improved from 0.05536 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0298 - val_loss: 0.0554\n","Epoch 429/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 429: val_loss improved from 0.05536 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0298 - val_loss: 0.0554\n","Epoch 430/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 430: val_loss improved from 0.05536 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0297 - val_loss: 0.0554\n","Epoch 431/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 431: val_loss improved from 0.05536 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0297 - val_loss: 0.0554\n","Epoch 432/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 432: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0297 - val_loss: 0.0554\n","Epoch 433/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 433: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0297 - val_loss: 0.0554\n","Epoch 434/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 434: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0297 - val_loss: 0.0554\n","Epoch 435/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 435: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0296 - val_loss: 0.0553\n","Epoch 436/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 436: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0296 - val_loss: 0.0553\n","Epoch 437/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 437: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0296 - val_loss: 0.0553\n","Epoch 438/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 438: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0296 - val_loss: 0.0553\n","Epoch 439/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 439: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0296 - val_loss: 0.0553\n","Epoch 440/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 440: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0295 - val_loss: 0.0553\n","Epoch 441/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 441: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0295 - val_loss: 0.0553\n","Epoch 442/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 442: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0295 - val_loss: 0.0553\n","Epoch 443/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 443: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0295 - val_loss: 0.0553\n","Epoch 444/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 444: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0295 - val_loss: 0.0553\n","Epoch 445/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 445: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 446/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 446: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 447/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 447: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 448/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 448: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0294 - val_loss: 0.0554\n","Epoch 449/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 449: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0294 - val_loss: 0.0554\n","Epoch 450/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 450: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0294 - val_loss: 0.0554\n","Epoch 451/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 451: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0293 - val_loss: 0.0554\n","Epoch 452/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 452: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0293 - val_loss: 0.0554\n","Epoch 453/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 453: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0293 - val_loss: 0.0554\n","Epoch 454/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 454: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0293 - val_loss: 0.0554\n","Epoch 455/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 455: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0293 - val_loss: 0.0554\n","Epoch 456/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 456: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0293 - val_loss: 0.0554\n","Epoch 457/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 457: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0293 - val_loss: 0.0554\n","Epoch 458/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 458: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0292 - val_loss: 0.0554\n","Epoch 459/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 459: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0292 - val_loss: 0.0554\n","Epoch 460/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 460: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0292 - val_loss: 0.0554\n","Epoch 461/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 461: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0292 - val_loss: 0.0554\n","Epoch 462/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 462: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0292 - val_loss: 0.0554\n","Epoch 463/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 463: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0292 - val_loss: 0.0554\n","Epoch 464/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 464: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0291 - val_loss: 0.0554\n","Epoch 465/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 465: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0291 - val_loss: 0.0554\n","Epoch 466/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 466: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0291 - val_loss: 0.0554\n","Epoch 467/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 467: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0291 - val_loss: 0.0554\n","Epoch 468/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 468: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0291 - val_loss: 0.0554\n","Epoch 469/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 469: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0291 - val_loss: 0.0554\n","Epoch 470/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 470: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0291 - val_loss: 0.0554\n","Epoch 471/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 471: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0291 - val_loss: 0.0554\n","Epoch 472/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 472: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0290 - val_loss: 0.0554\n","Epoch 473/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 473: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0290 - val_loss: 0.0554\n","Epoch 474/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 474: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0290 - val_loss: 0.0554\n","Epoch 475/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 475: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0290 - val_loss: 0.0554\n","Epoch 476/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 476: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0290 - val_loss: 0.0554\n","Epoch 477/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 477: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0290 - val_loss: 0.0554\n","Epoch 478/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 478: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0290 - val_loss: 0.0554\n","Epoch 479/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 479: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0290 - val_loss: 0.0554\n","Epoch 480/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 480: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0290 - val_loss: 0.0554\n","Epoch 481/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 481: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 482/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 482: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 483/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 483: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 484/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 484: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 485/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 485: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 486/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 486: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 487/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 487: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 488/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 488: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0289 - val_loss: 0.0555\n","Epoch 489/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 489: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0289 - val_loss: 0.0555\n","Epoch 490/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 490: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 491/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 491: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 492/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 492: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 493/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 493: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 494/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 494: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 495/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 495: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 496/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 496: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 497/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 497: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 498/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 498: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 499/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 499: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 500/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 500: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 501/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 501: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 502/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 502: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 503/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 503: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 504/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 504: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 505/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 505: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 506/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 506: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 507/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 507: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 508/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 508: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 509/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 509: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 510/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 510: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 511/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 511: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 512/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 512: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 513/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 513: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 514/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 514: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 515/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 515: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 516/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 516: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 517/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 517: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 518/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 518: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 519/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 519: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 520/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 520: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 521/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 521: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 522/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 522: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 523/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 523: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 524/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 524: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 525/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 525: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 526/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 526: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 527/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 527: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 528/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 528: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 529/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 529: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 530/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 530: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 531/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 531: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 532/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 532: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 533/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 533: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 534/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 534: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 535/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 535: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 536/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 536: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 537/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 537: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 538/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 538: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 539/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 539: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 540/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 540: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 541/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 541: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 542/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 542: val_loss did not improve from 0.05535\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0285 - val_loss: 0.0557\n","1/1 [==============================] - 0s 125ms/step - loss: 0.0734\n","loss_and_metrics : 0.07339371740818024\n","1/1 [==============================] - 0s 181ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","from tensorflow.keras.optimizers import Adam\n","from keras.optimizers import Nadam\n","from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=100, input_dim=40, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","#model.compile(loss='mean_squared_error', optimizer='adamax')\n","#model.compile(loss='mean_squared_error', optimizer='nadam')\n","#model.compile(loss='mean_squared_error', optimizer='rmsprop')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","history = model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","x_len = numpy.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]}]}